{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器翻译\n",
    "\n",
    "大家应该都使用过谷歌翻译或者百度翻译或者有道词典等等翻译工具，它们内部的技术就是机器翻译，通过训练深度神经网络来让机器学会一门语言翻译能力。\n",
    "\n",
    "本次实战编程实现的神经网络也可以用实现语言翻译，但是需要非常非常大的训练数据集和非常非常长的训练时间，所以处于教学目的，本次我们只用它来实现日期格式的翻译。日期的格式有很多种，仅仅是英文日期都有很多种表现格式，例如\"the 29th of August 1958\", \"03/30/1968\", \"24 JUNE 1987\"等等。一般来说电脑里的时间格式是\"2009-06-25\"，我们在这里将其称为电脑日期格式，将其它类型的格式称为人类日期格式。本次实战编程的目的就是要通过神经网络来将人类日期格式翻译成电脑日期格式。其实这个与英语翻译成汉语是同一个道理，只不过简单一些而已，仅仅需要少量的训练集就可以搞定了。\n",
    "\n",
    "\n",
    "首先按照下面的指令安装新的工具库。\n",
    "\n",
    "1，打开Anaconda prompt\n",
    "\n",
    "2，执行activate tensorflow命令\n",
    "\n",
    "3，执行pip install faker==2.0.3命令\n",
    "\n",
    "4，执行pip install tqdm命令\n",
    "\n",
    "5，执行pip install babel命令"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - 翻译时间格式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - 数据集\n",
    "\n",
    "下面的代码将加载10000个样本的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10000/10000 [00:01<00:00, 5800.74it/s]\n"
     ]
    }
   ],
   "source": [
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9 may 1998', '1998-05-09'),\n",
       " ('10.11.19', '2019-11-10'),\n",
       " ('9/10/70', '1970-09-10'),\n",
       " ('saturday april 28 1990', '1990-04-28'),\n",
       " ('thursday january 26 1995', '1995-01-26'),\n",
       " ('monday march 7 1983', '1983-03-07'),\n",
       " ('sunday may 22 1988', '1988-05-22'),\n",
       " ('08 jul 2008', '2008-07-08'),\n",
       " ('8 sep 1999', '1999-09-08'),\n",
       " ('thursday january 1 1981', '1981-01-01')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `dataset`: 这是一个tuples类型的list列表，每一项就是一个时间格式对（人类日期格式，电脑时间格式）,上面的代码打印出了前面10个时间格式对。\n",
    "- `human_vocab`: 这是一个字典，用于将人类日期格式里面的每一个字符转换成一个对应的数字索引。\n",
    "- `machine_vocab`: 这是一个字典，用于将电脑日期格式里面的每一个字符转换成一个对应的数字索引。注意，这个索引与上面的人类日期的索引不需要一一对应。\n",
    "- `inv_machine_vocab`: 这是一个字典，作用是与machine_vocab相反的，就是将索引转换成字符。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (10000, 30)\n",
      "Y.shape: (10000, 10)\n",
      "Xoh.shape: (10000, 30, 37)\n",
      "Yoh.shape: (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "# 下面的代码会将数据集里面的字符都转换成索引格式，并且拆分出X和Y，同时转换成one-hot形式。\n",
    "Tx = 30 # 这里是假设在人类日期格式中最多有30个字符，如果超过30，那么会被截断。\n",
    "Ty = 10 # 电脑格式\"YYYY-MM-DD\"中字符数量是固定的，就是10个。\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `X`: 输入X。这里面的人类格式日期的字符都是以索引来表示的了，而且每个日期都加了padding或被截断了，以保证长度都是30，所以X的维度是`X.shape = (m, Tx)`\n",
    "- `Y`: 真实标签Y。这里面的电脑格式日期的字符也都是以索引来表示的了。维度是`Y.shape = (m, Ty)`. \n",
    "- `Xoh`: X的one-hot版本。也就是用一个长度为len(human_vocab)的one-hot向量来表示某个字符。所以维度是 `Xoh.shape = (m, Tx, len(human_vocab))`\n",
    "- `Yoh`: Y的one-hot版本。维度是 `Yoh.shape = (m, Tx, len(machine_vocab))`. 这里的`len(machine_vocab) = 11`因为电脑格式日期中只有'-'和0-9这11个字符。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的代码将某个日期的不同表现方式打印出来了，有助于大家理解X和Xoh等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source date: 9 may 1998\n",
      "Target date: 1998-05-09\n",
      "\n",
      "Source after preprocessing (indices): [12  0 24 13 34  0  4 12 12 11 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n",
      " 36 36 36 36 36 36]\n",
      "Target after preprocessing (indices): [ 2 10 10  9  0  1  6  0  1 10]\n",
      "\n",
      "Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "Target after preprocessing (one-hot): [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(\"Source date:\", dataset[index][0])\n",
    "print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X[index])\n",
    "print(\"Target after preprocessing (indices):\", Y[index])\n",
    "print()\n",
    "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - 使用注意力模型来实现机器翻译\n",
    "\n",
    "当我翻译一篇英文文档时，我通常是先将文档看一遍，然后再从头开始一段一段的进行翻译。也就是说，当我进行翻译时，我会把大部分的注意力集中在当前要翻译的部分上。我们学过的注意力模型也是这个机制，它会告诉神经网络模型在翻译的某一个时刻应该把注意力放在句子的哪些部位上。\n",
    "\n",
    "\n",
    "### 2.1 - 注意力机制\n",
    "\n",
    "下面的左图是一个注意力模型，右图展示了在某一个解码网络的时间步时，与其相关的注意力权重$\\alpha^{\\langle t, t' \\rangle}$是如何被计算出来的。最后这些注意力权重和相应的编码网络的激活值a会组成一个变量$context^{\\langle t \\rangle}$，然后输入到解码网络的这个时间步中。这里要再次提醒大家要区分注意力权重$\\alpha$和激活值a！另外要注意的是我们使用t表示解码网络中的时间步编号，使用t'表示编码网络中的时间步编号。\n",
    "\n",
    "<table>\n",
    "<td> \n",
    "<img src=\"images/attn_model.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "<td> \n",
    "<img src=\"images/attn_mechanism.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "</table>\n",
    "<caption><center> **图 1**</center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是一些对上图的解释: \n",
    "\n",
    "- 左图中有两个LSTM，下面的双向LSTM是编码网络，由于它在注意力机制之前，所以也可以称为*pre-attention* Bi-LSTM。上面的LSTM是解码网络，因为在注意力机制后面，所以也被称为*post-attention* LSTM。编码网络有$T_x$个时间步，本例中是30; 解码网络中有$T_y$个时间步。 \n",
    "\n",
    "- 我们知道LSTM的每个时间步都会产生一个激活值$s^{\\langle t \\rangle}$和一个$c^{\\langle t \\rangle}$。如果不懂我在说什么，那么请复习一下前面关于LSTM的文章（5.1.10）。在前面的文章中因为我们只使用了一个简单的RNN来举例，所以那时使用$c^{\\langle t \\rangle}$来表示上图中的$context^{\\langle t \\rangle}$，但是在本文档中，我们使用了LSTM，所以就会出现两个c，为了避免冲突，本文档中使用了$context^{\\langle t \\rangle}$。另外，由于日期中各字符的关联不是很大，所以在解码网络中没有将上一个时间步的输出值$y^{\\langle t-1 \\rangle}$传递到下一个时间步去。\n",
    "\n",
    "- 编码网络是个双向的网络，所以$a^{\\langle t \\rangle} = [\\overrightarrow{a}^{\\langle t \\rangle}; \\overleftarrow{a}^{\\langle t \\rangle}]$,也就是包含了一个正向的激活值和反向的激活值。\n",
    "\n",
    "- 右图中使用了两个Keras的函数。使用`RepeatVector`来将$s^{\\langle t-1 \\rangle}$'复制了 $T_x$次，然后使用`Concatenation`来将$s^{\\langle t-1 \\rangle}$和$a^{\\langle t \\rangle}$结合成变量$e^{\\langle t, t'}$,最后输入到一个小型神经网络中来产生注意力权重$\\alpha^{\\langle t, t' \\rangle}$。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了实现右图，我们定义一些全局操作对象，在后面的函数中就可以多次调用这些对象了。\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "# 这个densor对象代表了一个dense网络层，\n",
    "# 由于这个对象是全局的，所以在每次调用它时，都是在训练同一套相关参数，这正是我们需要的。\n",
    "# 因为不停地训练同一套参数，那么网络就会越来越聪明，输出的注意力权重就会越来越合理。\n",
    "densor = Dense(1, activation = \"relu\") \n",
    "activator = Activation(softmax, name='attention_weights') \n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现上面右图的功能，也就是为解码网络的某一个时间步计算相应的context输入\n",
    "\n",
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"    \n",
    "    参数:\n",
    "    a -- 编码网络的所有时间步的激活值，维度是(m, Tx, 2*n_a)，这里n_a是神经元个数，乘以2是因为是双向网络。\n",
    "    s_prev -- 解码网络中前一个时间步激活值，维度是 (m, n_s)\n",
    "    \n",
    "    返回值:\n",
    "    context -- 这个结果会输入到解码网络的时间步中去。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 使用repeator对象将s_prev复制出多份来，使其维度成为 (m, Tx, n_s)，这样一来才能与激活值a进行组合 \n",
    "    s_prev = repeator(s_prev)\n",
    "    # 使用concatenator对象将a和s_prev连接起来\n",
    "    concat = concatenator([a, s_prev])\n",
    "    # 将其传入到一个全连接网络层中去，得到注意力权重alphas\n",
    "    e = densor(concat)\n",
    "    alphas = activator(e)\n",
    "    # 将激活值和注意力权重相乘然后相加，得到context\n",
    "    context = dotor([alphas, a])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一些构成模型时需要的全局对象，同理，在后面对这些对象进行调用时，始终在训练着同一套参数，这正是我们需要的\n",
    "# 因为每个时间步的参数应该是共享一套的。\n",
    "n_a = 64\n",
    "n_s = 128\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "output_layer = Dense(len(machine_vocab), activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现前面的左图。\n",
    "\n",
    "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    Tx -- 输入句子的最大长度，本文档我们使用30\n",
    "    Ty -- 输出句子的最大长度，是11\n",
    "    n_a -- 编码网络中每个LSTM的神经元个数\n",
    "    n_s -- 解码网络中每个LSTM的神经元个数\n",
    "    human_vocab_size -- 人类格式日期的词表的大小\n",
    "    machine_vocab_size -- 电脑格式日期的词表的大小\n",
    "\n",
    "    返回值:\n",
    "    model -- Keras模型实例\n",
    "    \"\"\"\n",
    "    \n",
    "    X = Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    # 下面这一句简短的代码就将编码网络给构建好了。此时你会再次感叹，Keras太方便了。\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences=True))(X)\n",
    "    \n",
    "    # 用for循环来执行解码网络中的每一个时间步\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # 调用前面我们实现的one_step_attention函数来为当前时间步计算出相应的context输入。\n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        # 执行这个时间步。得出新的s和c\n",
    "        s, _, c = post_activation_LSTM_cell(context, initial_state = [s, c])\n",
    "        \n",
    "        # 得出一个时间步的预测值\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        outputs.append(out)\n",
    "    \n",
    "    # 创建keras实例\n",
    "    model = Model(inputs = [X, s0, c0], outputs = outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的代码会输出这个模型的一些信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 30, 37)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "s0 (InputLayer)                  (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 30, 128)       52224       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)   (None, 30, 128)       0           s0[0][0]                         \n",
      "                                                                   lstm_1[0][0]                     \n",
      "                                                                   lstm_1[1][0]                     \n",
      "                                                                   lstm_1[2][0]                     \n",
      "                                                                   lstm_1[3][0]                     \n",
      "                                                                   lstm_1[4][0]                     \n",
      "                                                                   lstm_1[5][0]                     \n",
      "                                                                   lstm_1[6][0]                     \n",
      "                                                                   lstm_1[7][0]                     \n",
      "                                                                   lstm_1[8][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 30, 256)       0           bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[0][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[1][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[2][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[3][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[4][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[5][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[6][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[7][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[8][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 30, 1)         257         concatenate_1[0][0]              \n",
      "                                                                   concatenate_1[1][0]              \n",
      "                                                                   concatenate_1[2][0]              \n",
      "                                                                   concatenate_1[3][0]              \n",
      "                                                                   concatenate_1[4][0]              \n",
      "                                                                   concatenate_1[5][0]              \n",
      "                                                                   concatenate_1[6][0]              \n",
      "                                                                   concatenate_1[7][0]              \n",
      "                                                                   concatenate_1[8][0]              \n",
      "                                                                   concatenate_1[9][0]              \n",
      "____________________________________________________________________________________________________\n",
      "attention_weights (Activation)   (None, 30, 1)         0           dense_1[0][0]                    \n",
      "                                                                   dense_1[1][0]                    \n",
      "                                                                   dense_1[2][0]                    \n",
      "                                                                   dense_1[3][0]                    \n",
      "                                                                   dense_1[4][0]                    \n",
      "                                                                   dense_1[5][0]                    \n",
      "                                                                   dense_1[6][0]                    \n",
      "                                                                   dense_1[7][0]                    \n",
      "                                                                   dense_1[8][0]                    \n",
      "                                                                   dense_1[9][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dot_1 (Dot)                      (None, 1, 128)        0           attention_weights[0][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[1][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[2][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[3][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[4][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[5][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[6][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[7][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[8][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[9][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "c0 (InputLayer)                  (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    [(None, 128), (None,  131584      dot_1[0][0]                      \n",
      "                                                                   s0[0][0]                         \n",
      "                                                                   c0[0][0]                         \n",
      "                                                                   dot_1[1][0]                      \n",
      "                                                                   lstm_1[0][0]                     \n",
      "                                                                   lstm_1[0][2]                     \n",
      "                                                                   dot_1[2][0]                      \n",
      "                                                                   lstm_1[1][0]                     \n",
      "                                                                   lstm_1[1][2]                     \n",
      "                                                                   dot_1[3][0]                      \n",
      "                                                                   lstm_1[2][0]                     \n",
      "                                                                   lstm_1[2][2]                     \n",
      "                                                                   dot_1[4][0]                      \n",
      "                                                                   lstm_1[3][0]                     \n",
      "                                                                   lstm_1[3][2]                     \n",
      "                                                                   dot_1[5][0]                      \n",
      "                                                                   lstm_1[4][0]                     \n",
      "                                                                   lstm_1[4][2]                     \n",
      "                                                                   dot_1[6][0]                      \n",
      "                                                                   lstm_1[5][0]                     \n",
      "                                                                   lstm_1[5][2]                     \n",
      "                                                                   dot_1[7][0]                      \n",
      "                                                                   lstm_1[6][0]                     \n",
      "                                                                   lstm_1[6][2]                     \n",
      "                                                                   dot_1[8][0]                      \n",
      "                                                                   lstm_1[7][0]                     \n",
      "                                                                   lstm_1[7][2]                     \n",
      "                                                                   dot_1[9][0]                      \n",
      "                                                                   lstm_1[8][0]                     \n",
      "                                                                   lstm_1[8][2]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 11)            1419        lstm_1[0][0]                     \n",
      "                                                                   lstm_1[1][0]                     \n",
      "                                                                   lstm_1[2][0]                     \n",
      "                                                                   lstm_1[3][0]                     \n",
      "                                                                   lstm_1[4][0]                     \n",
      "                                                                   lstm_1[5][0]                     \n",
      "                                                                   lstm_1[6][0]                     \n",
      "                                                                   lstm_1[7][0]                     \n",
      "                                                                   lstm_1[8][0]                     \n",
      "                                                                   lstm_1[9][0]                     \n",
      "====================================================================================================\n",
      "Total params: 185,484\n",
      "Trainable params: 185,484\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#编译这个模型\n",
    "out = model.compile(optimizer=Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01),\n",
    "                    metrics=['accuracy'],\n",
    "                    loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 49s - loss: 3.5670 - dense_2_loss_1: 0.0603 - dense_2_loss_2: 0.0542 - dense_2_loss_3: 0.3003 - dense_2_loss_4: 0.6077 - dense_2_loss_5: 0.0057 - dense_2_loss_6: 0.0908 - dense_2_loss_7: 0.8481 - dense_2_loss_8: 0.0056 - dense_2_loss_9: 0.5972 - dense_2_loss_10: 0.9971 - dense_2_acc_1: 0.9767 - dense_2_acc_2: 0.9777 - dense_2_acc_3: 0.8784 - dense_2_acc_4: 0.7991 - dense_2_acc_5: 1.0000 - dense_2_acc_6: 0.9721 - dense_2_acc_7: 0.7160 - dense_2_acc_8: 1.0000 - dense_2_acc_9: 0.7629 - dense_2_acc_10: 0.6296    \n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 49s - loss: 2.6884 - dense_2_loss_1: 0.0483 - dense_2_loss_2: 0.0415 - dense_2_loss_3: 0.2513 - dense_2_loss_4: 0.3833 - dense_2_loss_5: 0.0045 - dense_2_loss_6: 0.0785 - dense_2_loss_7: 0.6556 - dense_2_loss_8: 0.0046 - dense_2_loss_9: 0.4952 - dense_2_loss_10: 0.7255 - dense_2_acc_1: 0.9810 - dense_2_acc_2: 0.9826 - dense_2_acc_3: 0.8889 - dense_2_acc_4: 0.8849 - dense_2_acc_5: 1.0000 - dense_2_acc_6: 0.9769 - dense_2_acc_7: 0.7926 - dense_2_acc_8: 0.9998 - dense_2_acc_9: 0.8131 - dense_2_acc_10: 0.7365    \n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 47s - loss: 2.0952 - dense_2_loss_1: 0.0399 - dense_2_loss_2: 0.0329 - dense_2_loss_3: 0.2149 - dense_2_loss_4: 0.2718 - dense_2_loss_5: 0.0035 - dense_2_loss_6: 0.0739 - dense_2_loss_7: 0.4991 - dense_2_loss_8: 0.0038 - dense_2_loss_9: 0.4160 - dense_2_loss_10: 0.5393 - dense_2_acc_1: 0.9842 - dense_2_acc_2: 0.9872 - dense_2_acc_3: 0.9034 - dense_2_acc_4: 0.9172 - dense_2_acc_5: 1.0000 - dense_2_acc_6: 0.9750 - dense_2_acc_7: 0.8548 - dense_2_acc_8: 1.0000 - dense_2_acc_9: 0.8475 - dense_2_acc_10: 0.8097    \n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 46s - loss: 1.6862 - dense_2_loss_1: 0.0328 - dense_2_loss_2: 0.0264 - dense_2_loss_3: 0.1894 - dense_2_loss_4: 0.2162 - dense_2_loss_5: 0.0031 - dense_2_loss_6: 0.0594 - dense_2_loss_7: 0.3807 - dense_2_loss_8: 0.0036 - dense_2_loss_9: 0.3508 - dense_2_loss_10: 0.4236 - dense_2_acc_1: 0.9883 - dense_2_acc_2: 0.9905 - dense_2_acc_3: 0.9140 - dense_2_acc_4: 0.9320 - dense_2_acc_5: 1.0000 - dense_2_acc_6: 0.9809 - dense_2_acc_7: 0.8923 - dense_2_acc_8: 1.0000 - dense_2_acc_9: 0.8732 - dense_2_acc_10: 0.8481    \n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 46s - loss: 1.4298 - dense_2_loss_1: 0.0284 - dense_2_loss_2: 0.0215 - dense_2_loss_3: 0.1651 - dense_2_loss_4: 0.1801 - dense_2_loss_5: 0.0026 - dense_2_loss_6: 0.0541 - dense_2_loss_7: 0.3111 - dense_2_loss_8: 0.0032 - dense_2_loss_9: 0.3050 - dense_2_loss_10: 0.3587 - dense_2_acc_1: 0.9899 - dense_2_acc_2: 0.9922 - dense_2_acc_3: 0.9269 - dense_2_acc_4: 0.9460 - dense_2_acc_5: 1.0000 - dense_2_acc_6: 0.9813 - dense_2_acc_7: 0.9142 - dense_2_acc_8: 1.0000 - dense_2_acc_9: 0.8896 - dense_2_acc_10: 0.8738    \n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 46s - loss: 1.2530 - dense_2_loss_1: 0.0236 - dense_2_loss_2: 0.0180 - dense_2_loss_3: 0.1451 - dense_2_loss_4: 0.1551 - dense_2_loss_5: 0.0021 - dense_2_loss_6: 0.0491 - dense_2_loss_7: 0.2691 - dense_2_loss_8: 0.0030 - dense_2_loss_9: 0.2681 - dense_2_loss_10: 0.3198 - dense_2_acc_1: 0.9923 - dense_2_acc_2: 0.9930 - dense_2_acc_3: 0.9416 - dense_2_acc_4: 0.9540 - dense_2_acc_5: 1.0000 - dense_2_acc_6: 0.9826 - dense_2_acc_7: 0.9256 - dense_2_acc_8: 0.9999 - dense_2_acc_9: 0.9019 - dense_2_acc_10: 0.8838    \n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 46s - loss: 1.0959 - dense_2_loss_1: 0.0199 - dense_2_loss_2: 0.0144 - dense_2_loss_3: 0.1253 - dense_2_loss_4: 0.1321 - dense_2_loss_5: 0.0019 - dense_2_loss_6: 0.0471 - dense_2_loss_7: 0.2372 - dense_2_loss_8: 0.0026 - dense_2_loss_9: 0.2322 - dense_2_loss_10: 0.2832 - dense_2_acc_1: 0.9935 - dense_2_acc_2: 0.9952 - dense_2_acc_3: 0.9508 - dense_2_acc_4: 0.9624 - dense_2_acc_5: 1.0000 - dense_2_acc_6: 0.9828 - dense_2_acc_7: 0.9333 - dense_2_acc_8: 1.0000 - dense_2_acc_9: 0.9196 - dense_2_acc_10: 0.8928    \n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 48s - loss: 0.9836 - dense_2_loss_1: 0.0168 - dense_2_loss_2: 0.0110 - dense_2_loss_3: 0.1113 - dense_2_loss_4: 0.1171 - dense_2_loss_5: 0.0017 - dense_2_loss_6: 0.0421 - dense_2_loss_7: 0.2133 - dense_2_loss_8: 0.0026 - dense_2_loss_9: 0.2069 - dense_2_loss_10: 0.2606 - dense_2_acc_1: 0.9950 - dense_2_acc_2: 0.9962 - dense_2_acc_3: 0.9574 - dense_2_acc_4: 0.9687 - dense_2_acc_5: 1.0000 - dense_2_acc_6: 0.9851 - dense_2_acc_7: 0.9406 - dense_2_acc_8: 1.0000 - dense_2_acc_9: 0.9286 - dense_2_acc_10: 0.9010    \n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 49s - loss: 0.8918 - dense_2_loss_1: 0.0143 - dense_2_loss_2: 0.0088 - dense_2_loss_3: 0.1007 - dense_2_loss_4: 0.1052 - dense_2_loss_5: 0.0015 - dense_2_loss_6: 0.0406 - dense_2_loss_7: 0.1919 - dense_2_loss_8: 0.0022 - dense_2_loss_9: 0.1835 - dense_2_loss_10: 0.2431 - dense_2_acc_1: 0.9963 - dense_2_acc_2: 0.9977 - dense_2_acc_3: 0.9605 - dense_2_acc_4: 0.9736 - dense_2_acc_5: 1.0000 - dense_2_acc_6: 0.9849 - dense_2_acc_7: 0.9433 - dense_2_acc_8: 1.0000 - dense_2_acc_9: 0.9405 - dense_2_acc_10: 0.9028    \n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 48s - loss: 0.8213 - dense_2_loss_1: 0.0121 - dense_2_loss_2: 0.0072 - dense_2_loss_3: 0.0907 - dense_2_loss_4: 0.0962 - dense_2_loss_5: 0.0015 - dense_2_loss_6: 0.0370 - dense_2_loss_7: 0.1817 - dense_2_loss_8: 0.0020 - dense_2_loss_9: 0.1672 - dense_2_loss_10: 0.2257 - dense_2_acc_1: 0.9972 - dense_2_acc_2: 0.9984 - dense_2_acc_3: 0.9672 - dense_2_acc_4: 0.9808 - dense_2_acc_5: 1.0000 - dense_2_acc_6: 0.9872 - dense_2_acc_7: 0.9463 - dense_2_acc_8: 1.0000 - dense_2_acc_9: 0.9487 - dense_2_acc_10: 0.9114    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c129303d68>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([Xoh, s0, c0], outputs, epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面用这个模型来翻译一些日期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: 3 May 1979\n",
      "output: 1979-05-03\n",
      "source: 5 April 09\n",
      "output: 2009-04-06\n",
      "source: 21th of August 2016\n",
      "output: 2016-08-14\n",
      "source: Tue 10 Jul 2007\n",
      "output: 2007-07-10\n",
      "source: Saturday May 9 2018\n",
      "output: 2018-05-09\n",
      "source: March 3 2001\n",
      "output: 2011-03-03\n",
      "source: March 3rd 2001\n",
      "output: 2011-03-03\n",
      "source: 1 March 2001\n",
      "output: 2011-03-01\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "for example in EXAMPLES:\n",
    "    \n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
    "    prediction = model.predict([source, s0, c0])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    \n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output))"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "n16CQ",
   "launcher_item_id": "npjGi"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
