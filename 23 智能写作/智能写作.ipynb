{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 智能写作\n",
    "\n",
    "感谢您学习床长人工智能教程!（ http://www.captainbed.net )\n",
    "\n",
    "智能写作就是指人工智能模型自己能根据一定的条件或者是无任何条件下自由地生成内容。例如人工智能写小说，写新闻，写诗等等。智能写作其实已经被应用到商业领域了，很多大出版商都研发了智能写作系统。可能你每天阅读的新闻或文章就能人工智能写的，只是你察觉不到而已。\n",
    "\n",
    "为了循序渐进地让大家弄懂智能写作程序的研发，我们先来实现一个智能起名程序，然后再来看一个智能写诗的程序。\n",
    "\n",
    "本次实战编程中我们将实现一个为恐龙起名的程序，这个程序是一个字（母）级别的RNN模型，也就是RNN的每个时间步只输出一个字（母）。因为汉字太多，较复杂，而教学目的就是要简单易懂，所以我们选择实现英文版的，英文总共就26个字母。学会了英文字母版的后，如果感兴趣的话你也可自行实现汉字版的。生活中有些人会去算命先生那里为小孩起名,貌似现在还有专门起名的网站。\n",
    "\n",
    "\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/dino.jpg\" style=\"width:250;height:300px;\">\n",
    "\n",
    "</td>\n",
    "\n",
    "</table>\n",
    "\n",
    "我们已经为大家收集了很多恐龙的名字，将它们保存在了[dataset](dinos.txt)中，大家可以点击查看这个数据集。其实就是本文档同目录下的dinos.txt文件。你也可以用写字板或者word打开它（如果用记事本软件打开它，会出现无换行的情况）。最出名的恐龙名就是霸王龙了，想必不少人都看过《侏罗纪公园》，霸王龙相当地牛逼啊，它的英文是Tyrannosaurus，你可以在dinos.txt中搜索到Tyrannosaurus。迅猛龙的英文是Velociraptor，迅猛龙也是非常吓人的~ 下面我们将构建一个字母级别的语言模型，来学习dinos.txt中的恐龙名，找到为恐龙命名的规律，以获得生成出新恐龙名的能力。 \n",
    "\n",
    "首先我们加载一些系统库以及我们自定义的工具库`utils`，这个工具库里面已经为大家实现了很多函数，包括`rnn_forward` 和`rnn_backward`等等，这些函数的实现我们在前一个实战编程中已经学习了，所以在此就不为它们浪费篇幅了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1 - 数据预处理\n",
    "\n",
    "下面的代码单元会读取出dinos.txt文件中所有的恐龙名，并且从中提取出一个信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dinos.txt文件里面总共有19909个字符，27个字符种类\n"
     ]
    }
   ],
   "source": [
    "data = open('dinos.txt', 'r').read() # 读取出所有恐龙名\n",
    "data = data.lower() # 都转换成小写字母\n",
    "chars = list(set(data))# 提取出这些名字中的字符种类（英文中总共就26个字母，加上文件中的换行符，就是27个字符） \n",
    "data_size, vocab_size = len(data), len(chars) # 提取出字符数量，以及字符种类数量\n",
    "print('dinos.txt文件里面总共有%d个字符，%d个字符种类' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们前面的文章中说过`<EOS>`可以表示句子的末尾，在本数据集中我们使用换行符来充当着`<EOS>`的作用，用它来表示一个恐龙名的末尾。换行符在编程中是用\"\\n\"来表示的，程序员一般都知道。\n",
    "\n",
    "下面的代码单元会生成两个python字典，字典中每个字符种类都对应了一个索引，索引范围是[0,26]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) } # 这个字典用于将字符种类转化成索引\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) } # 这个字典用于将索引转化成字符种类\n",
    "print(ix_to_char) # 从下面的打印结果可知，索引0代表了换行符，字母z的索引是26...  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - 实现2个功能函数\n",
    "\n",
    "下面我将实现两个功能函数，它们将会在后面我们构建神经网络模型时起到大作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - 梯度值裁剪函数\n",
    "\n",
    "后面我们将在模型中使用这个函数来避免RNN出现梯度爆炸。有很多种裁剪的方法，本文档中我们将使用一种简单的方法，那就是为梯度值设置一个最大值，如果实际梯度值超过了这个最大值，那么我们就将梯度值设为这个最大值，也就是说，绝对不会让梯度值超过这个最大值。假设我们将最大值设置为10，如果梯度值超过10，就将梯度值设置为10，如果梯度值小于-10，那么就将梯度值设置为-10.\n",
    "\n",
    "我们知道如果出现梯度爆炸，那么会严重影响神经网络的学习效率。下面左图就是一个出现梯度爆炸的神经网络的学习路线图，可以看出神经网络一整乱撞，经常偏离代表最优解的中心点。而右图就是经过了梯度值裁剪的神经网络的学习路线图，不会到处乱跑，而是一步步地逼近最优解。\n",
    "<img src=\"images/clip.png\" style=\"width:400;height:150px;\">\n",
    "<caption><center> **图 2**</center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 梯度值裁剪函数\n",
    "\n",
    "def clip(gradients, maxValue):\n",
    "    '''    \n",
    "    参数:\n",
    "    gradients -- 一个字典，包含了如下梯度值\"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
    "    maxValue -- 指定的最大值\n",
    "    \n",
    "    返回值: \n",
    "    gradients -- 一个字典，包含了被裁剪后的梯度值\n",
    "    '''\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "\n",
    "    # 循环取出每组梯度值\n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        np.clip(gradient, -maxValue, maxValue, out=gradient) # 裁剪这组梯度值\n",
    "    \n",
    "    # 将裁剪后的梯度值重新保存到gradients中\n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dWaa\"][1][2] = 10.0\n",
      "gradients[\"dWax\"][3][1] = -10.0\n",
      "gradients[\"dWya\"][1][2] = 0.2971381536101662\n",
      "gradients[\"db\"][4] = [10.]\n",
      "gradients[\"dby\"][1] = [8.45833407]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "dWax = np.random.randn(5,3)*10\n",
    "dWaa = np.random.randn(5,5)*10\n",
    "dWya = np.random.randn(2,5)*10\n",
    "db = np.random.randn(5,1)*10\n",
    "dby = np.random.randn(2,1)*10\n",
    "gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "gradients = clip(gradients, 10)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - 采样函数\n",
    "\n",
    "教程前面的文章中我们说过，语言模型的每一个时间步输出的预测值是词表中每个词的概率。如下图所示，在本文档中每个时间步输出的预测值就是每个字母的概率。例如第一个时间步输出的预测值中字母a的概率可能是10%，字母m的概率是2%等等,第二个时间步输出的预测值中字母a的概率可能是5%，字母m的概率是13%等等。一个模型训练好后，参数都固定了，那么只要输入是固定的，输出就也是固定的，例如在人脸识别中，输出张三的脸，那么神经网络的预测结果肯定是张三，不可能出现有时候预测成张三有时候会是李四的情况。那么问题就来了，当语言模型训练好了后，我们如何让它生成恐龙名字呢？如果每次都取每个时间步的最大概率的那个字母，那岂不是每次模型生成的都是同一个名字~ 所以我们需要采样函数，不是每次都取最大概率的字母，而是随机往预测值里面取出一个字母来，这个随机取的过程就叫做采样(sample)。在下图中，每一个时间步得出预测值后都会进行一次采样，然后将采样得到的字母作为输入传给下一个时间步。一般来说，在模型训练完成后，在使用模型时才加入采样的功能，也就是说在模型训练期间是不进行采样的。但是我们本次实战编程的目的在于教学，所以我们会一边训练一边采样，每训练2000次就采样生成7个名字，这样能让我们看到随着训练次数的增加，采样输出的结果越来越好的现象。\n",
    "\n",
    "<img src=\"images/dinos3.png\" style=\"width:500;height:300px;\">\n",
    "<caption><center> **图 3**</center></caption>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 采样函数\n",
    "\n",
    "def sample(parameters, char_to_ix, seed):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    parameters -- 训练好了的参数，该字典包含了Waa, Wax, Wya, by, b. \n",
    "    char_to_ix -- 用于将字符转化为相应索引的字典\n",
    "    seed -- 随机种子\n",
    "\n",
    "    返回值:\n",
    "    indices -- 采样得到的一串字符的索引\n",
    "    \"\"\"\n",
    "    \n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    # 获取词表的大小。因为by是与预测值关联的阈值，那么预测值有多少个元素就有多少个b，\n",
    "    # 在本例中预测值包含了27个元素，也就是27个字符的概率。这27个字符就是我们的词表。\n",
    "    # 所以vocab_size就等于27.\n",
    "    vocab_size = by.shape[0] \n",
    "    # 获取RNN时间步单元中的神经元的个数。Waa是指用前一个时间步的激活值a作为输入来计算a时的参数。\n",
    "    # 因为一个神经元对应一个激活值，所以利用Waa就可以知道时间步单元中的神经元的个数.\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    \n",
    "    # 第一个时间步的输入x是一个0向量\n",
    "    # 输入x是一个one-hot向量，就是向量里只有一个元素是1，其它元素都是0.\n",
    "    # 这个one-hot向量的大小就是词表的大小，在本例中就是27。\n",
    "    # 如果输入x是字母a的话，那么向量中只有第二个元素是1，其它元素都是0.\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    # 第一个时间步的输入a也是0向量\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    indices = []\n",
    "    \n",
    "    idx = -1 \n",
    "\n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n'] # 获取换行符对应的索引\n",
    "    \n",
    "    # 不停地执行时间步来生成字母，直到生成字母是换行符时或者生成了50个字母了才结束。\n",
    "    # 对于一个训练有素的模型来说，不可能出现生成一个50个字母的恐龙名，名字哪有那么长，\n",
    "    # 但是为了防止出现无限循环，我们还是加上50个字母的上限设置。\n",
    "    while (idx != newline_character and counter != 50):\n",
    "        \n",
    "        # 执行一个时间步，得出预测值y，y里面保存了每个字母的概率\n",
    "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n",
    "        z = np.dot(Wya, a) + by\n",
    "        y = softmax(z)\n",
    "        \n",
    "        # 为每个时间步的采样设置不同的随机种子\n",
    "        np.random.seed(counter + seed) \n",
    "        \n",
    "        # 从y中随机选出一个元素，返回这个元素在词表中的索引到idx中\n",
    "        # choice的随机并不是纯粹地乱随机，它是根据y中每个元素的概率来选的。\n",
    "        # 假如y中字母h的概率为30%，那么choice就有30%的概率选择返回h的索引到idx中。\n",
    "        idx = np.random.choice(list(range(vocab_size)), p=y.ravel())\n",
    "\n",
    "        # 添加本时间步采样到的这个字母的索引到indices中。\n",
    "        indices.append(idx)\n",
    "        \n",
    "        # 将本时间步采样到的字母设置为下一个时间步的输入x\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[idx] = 1 # 因为x是一个one-hot向量，所以将idx对应的元素设置为1就可以让x代表本时间步采样到的字母了。\n",
    "        \n",
    "        # 将本时间步得到的激活值a设置为下一个时间步的输入a\n",
    "        a_prev = a\n",
    "        \n",
    "        seed += 1\n",
    "        counter +=1\n",
    "\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling:\n",
      "list of sampled indices: [12, 17, 24, 14, 13, 9, 10, 22, 24, 6, 13, 11, 12, 6, 21, 15, 21, 14, 3, 2, 1, 21, 18, 24, 7, 25, 6, 25, 18, 10, 16, 2, 3, 8, 15, 12, 11, 7, 1, 12, 10, 2, 7, 7, 11, 3, 6, 23, 13, 1, 0]\n",
      "list of sampled characters: ['l', 'q', 'x', 'n', 'm', 'i', 'j', 'v', 'x', 'f', 'm', 'k', 'l', 'f', 'u', 'o', 'u', 'n', 'c', 'b', 'a', 'u', 'r', 'x', 'g', 'y', 'f', 'y', 'r', 'j', 'p', 'b', 'c', 'h', 'o', 'l', 'k', 'g', 'a', 'l', 'j', 'b', 'g', 'g', 'k', 'c', 'f', 'w', 'm', 'a', '\\n']\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "_, n_a = 20, 100\n",
    "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "\n",
    "\n",
    "indices = sample(parameters, char_to_ix, 0)\n",
    "print(\"Sampling:\")\n",
    "print(\"list of sampled indices:\", indices)\n",
    "print(\"list of sampled characters:\", [ix_to_char[i] for i in indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - 构建语言模型\n",
    "\n",
    "下面我们来一步步地构建字母级别的语言模型\n",
    "\n",
    "\n",
    "### 3.1 - 优化函数\n",
    "\n",
    "下面我们将实现一个优化函数，在这个函数中我们会进行一次梯度下降，也就是执行一次前向传播，一次反向传播以及优化更新一次参数，这三个步骤我们之前已经学过了，所以为了节省篇幅我们在工具库里已经为大家实现了对应的函数，分别是rnn_forward，rnn_backward，update_parameters。本次我们使用的随机梯度下降，随机梯度下降的定义在《2.2.1 mini-batch》中——“如果你将一个样本当做一个子训练集的话，这就叫做随机梯度下降”。也就是说每次我们只输入一个训练样本，一个样本中只包含一个恐龙名。使用随机梯度下降时梯度值会乱跑，所以在优化函数中我们使用了裁剪函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化函数\n",
    "\n",
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    执行一次梯度下降\n",
    "    \n",
    "    参数:\n",
    "    X -- 包含了一个恐龙名的索引，例如可以是X = [None,3,5,11,22,3]，\n",
    "         对应于27个字符的词表就相当于X = [None,c,e,k,v,c]\n",
    "    Y -- 真实标签Y也是这个恐龙名，只不过与X错了一下位，X = [3,5,11,22,3，0]，最后一个0表示结尾。\n",
    "         为什么要错位呢，因为当第一个时间步输入空None时，我们希望这个时间步的预测结果是3（即希望3的概率最大），\n",
    "         当第二个时间步输入3时，我们希望这个时间步的预测结果是5.\n",
    "    a_prev -- 上一次梯度下降得到的激活值\n",
    "    parameters -- 参数字典:\n",
    "                        Wax -- (n_a, n_x)\n",
    "                        Waa -- (n_a, n_a)\n",
    "                        Wya -- (n_y, n_a)\n",
    "                        b --  (n_a, 1)\n",
    "                        by -- (n_y, 1)\n",
    "    \n",
    "    返回值:\n",
    "    loss -- 损失值\n",
    "    gradients -- 梯度字典:\n",
    "                        dWax -- (n_a, n_x)\n",
    "                        dWaa -- (n_a, n_a)\n",
    "                        dWya -- (n_y, n_a)\n",
    "                        db -- (n_a, 1)\n",
    "                        dby --(n_y, 1)\n",
    "    a[len(X)-1] -- 本次梯度下降得到的激活值，维度是 (n_a, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "    \n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    # 调用裁剪函数对梯度值进行裁剪\n",
    "    gradients = clip(gradients, 5)\n",
    "    \n",
    "    # 用裁剪后的梯度来优化参数\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 142.21675878802196\n",
      "gradients[\"dWaa\"][1][2] = -2.478417684052266\n",
      "np.argmax(gradients[\"dWax\"]) = 113\n",
      "gradients[\"dWya\"][1][2] = -0.9888264360547837\n",
      "gradients[\"db\"][4] = [5.]\n",
      "gradients[\"dby\"][1] = [0.98882644]\n",
      "a_last[4] = [-0.99994779]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "vocab_size, n_a = 27, 100\n",
    "a_prev = np.random.randn(n_a, 1)\n",
    "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "X = [None,3,5,11,22,3]\n",
    "Y = [3,5,11,22,3,0]\n",
    "\n",
    "loss, gradients, a_last = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "print(\"Loss =\", loss)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"np.argmax(gradients[\\\"dWax\\\"]) =\", np.argmax(gradients[\"dWax\"]))\n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])\n",
    "print(\"a_last[4] =\", a_last[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2 - 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27):\n",
    "    \"\"\"\n",
    "    训练模型，训练期间每训练2000次就采样生成7个名字\n",
    "    \n",
    "    参数:\n",
    "    ix_to_char -- 索引转字母的字典\n",
    "    char_to_ix -- 字母转索引的字典\n",
    "    num_iterations -- 需要训练的次数\n",
    "    n_a -- 设置RNN的神经元个数\n",
    "    dino_names -- 每次需要产生多少个名字 \n",
    "    vocab_size -- 词表大小，即字符种类数\n",
    "    \n",
    "    返回值:\n",
    "    parameters -- 训练得到的最终参数\n",
    "    \"\"\"\n",
    "    \n",
    "    # 获取输入x和输出y向量的大小。\n",
    "    # 因为输入x是one-hot向量，所以它的大小就等于词表的大小\n",
    "    # 因为输出y是关于词表中每一个字母的概率，所以大小也与词表相同\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    # 根据神经元个数以及输入元素个数和输出元素个数来创建并初始化相应的Waa，by等等相关参数\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    # 这个是用于在后面来使损失更加平滑的，不用太在意这个\n",
    "    loss = get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    # 将数据集dinos.txt中的所有恐龙名读取处理\n",
    "    with open(\"dinos.txt\") as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    \n",
    "    # 打乱这些名字的顺序\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # 进行训练\n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "        # 选出一个恐龙名来作为训练样本\n",
    "        index = j % len(examples) # 除以j是为了防止索引超出名字数量\n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]] \n",
    "        Y = X[1:] + [char_to_ix[\"\\n\"]] # 除了第一个元素None外，将所有X元素赋值给Y，然后再加上个换行符\n",
    "        \n",
    "        # 进行一次训练\n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters)\n",
    "        \n",
    "        # 是损失更加平滑\n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        # 每训练2000次生成7个名字\n",
    "        if j % 2000 == 0:\n",
    "            \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "            seed = 0\n",
    "            # 循环7次，生成7个名字\n",
    "            for name in range(dino_names): \n",
    "                \n",
    "                # 在当前训练好的模型上进行采样，生成一个名字\n",
    "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "                print_sample(sampled_indices, ix_to_char)\n",
    "                \n",
    "                seed += 1  # 增加采样的随机种子的值，避免每次生成的都是同样的名字\n",
    "      \n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行下面的代码后，训练会跑起来。你注意观察结果，你会发现开始的名字都不像是名字，随着训练次数的增加，生成的字符串越来越像名字了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 23.087336\n",
      "\n",
      "Nkzxwtdmfqoeyhsqwasjkjvu\n",
      "Kneb\n",
      "Kzxwtdmfqoeyhsqwasjkjvu\n",
      "Neb\n",
      "Zxwtdmfqoeyhsqwasjkjvu\n",
      "Eb\n",
      "Xwtdmfqoeyhsqwasjkjvu\n",
      "\n",
      "\n",
      "Iteration: 2000, Loss: 27.884160\n",
      "\n",
      "Liusskeomnolxeros\n",
      "Hmdaairus\n",
      "Hytroligoraurus\n",
      "Lecalosapaus\n",
      "Xusicikoraurus\n",
      "Abalpsamantisaurus\n",
      "Tpraneronxeros\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 25.901815\n",
      "\n",
      "Mivrosaurus\n",
      "Inee\n",
      "Ivtroplisaurus\n",
      "Mbaaisaurus\n",
      "Wusichisaurus\n",
      "Cabaselachus\n",
      "Toraperlethosdarenitochusthiamamumamaon\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 24.608779\n",
      "\n",
      "Onwusceomosaurus\n",
      "Lieeaerosaurus\n",
      "Lxussaurus\n",
      "Oma\n",
      "Xusteonosaurus\n",
      "Eeahosaurus\n",
      "Toreonosaurus\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 24.070350\n",
      "\n",
      "Onxusichepriuon\n",
      "Kilabersaurus\n",
      "Lutrodon\n",
      "Omaaerosaurus\n",
      "Xutrcheps\n",
      "Edaksoje\n",
      "Trodiktonus\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 23.844446\n",
      "\n",
      "Onyusaurus\n",
      "Klecalosaurus\n",
      "Lustodon\n",
      "Ola\n",
      "Xusodonia\n",
      "Eeaeosaurus\n",
      "Troceosaurus\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 23.291971\n",
      "\n",
      "Onyxosaurus\n",
      "Kica\n",
      "Lustrepiosaurus\n",
      "Olaagrraiansaurus\n",
      "Yuspangosaurus\n",
      "Eealosaurus\n",
      "Trognesaurus\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 23.382339\n",
      "\n",
      "Meutromodromurus\n",
      "Inda\n",
      "Iutroinatorsaurus\n",
      "Maca\n",
      "Yusteratoptititan\n",
      "Ca\n",
      "Troclosaurus\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 23.259291\n",
      "\n",
      "Meustomia\n",
      "Indaadps\n",
      "Justolongchudosatrus\n",
      "Macabosaurus\n",
      "Yuspanhosaurus\n",
      "Caaerosaurus\n",
      "Trodon\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 22.940799\n",
      "\n",
      "Phusaurus\n",
      "Meicamitheastosaurus\n",
      "Mussteratops\n",
      "Peg\n",
      "Ytrong\n",
      "Egaltor\n",
      "Trolome\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 22.894192\n",
      "\n",
      "Meutrodon\n",
      "Lledansteh\n",
      "Lwuspconyxauosaurus\n",
      "Macalosaurus\n",
      "Yusocichugus\n",
      "Eiagosaurus\n",
      "Trrangosaurus\n",
      "\n",
      "\n",
      "Iteration: 22000, Loss: 22.851820\n",
      "\n",
      "Onustolia\n",
      "Midcagosaurus\n",
      "Mwrrodonnonus\n",
      "Ola\n",
      "Yurodon\n",
      "Eiaeptia\n",
      "Trodoniohus\n",
      "\n",
      "\n",
      "Iteration: 24000, Loss: 22.700408\n",
      "\n",
      "Meutosaurus\n",
      "Jmacagosaurus\n",
      "Kurrodon\n",
      "Macaistel\n",
      "Yuroeleton\n",
      "Eiaeror\n",
      "Trodonosaurus\n",
      "\n",
      "\n",
      "Iteration: 26000, Loss: 22.736918\n",
      "\n",
      "Niutosaurus\n",
      "Liga\n",
      "Lustoingosaurus\n",
      "Necakroia\n",
      "Xrprinhtilus\n",
      "Eiaestehastes\n",
      "Trocilosaurus\n",
      "\n",
      "\n",
      "Iteration: 28000, Loss: 22.595568\n",
      "\n",
      "Meutosaurus\n",
      "Kolaaeus\n",
      "Kystodonisaurus\n",
      "Macahtopadrus\n",
      "Xtrrararkaumurpasaurus\n",
      "Eiaeosaurus\n",
      "Trodmanolus\n",
      "\n",
      "\n",
      "Iteration: 30000, Loss: 22.609381\n",
      "\n",
      "Meutosaurus\n",
      "Kracakosaurus\n",
      "Lustodon\n",
      "Macaisthachwisaurus\n",
      "Wusqandosaurus\n",
      "Eiacosaurus\n",
      "Trsatisaurus\n",
      "\n",
      "\n",
      "Iteration: 32000, Loss: 22.251308\n",
      "\n",
      "Mausinasaurus\n",
      "Incaadropeglsaurus\n",
      "Itrosaurus\n",
      "Macamisaurus\n",
      "Wuroenatoraerax\n",
      "Ehanosaurus\n",
      "Trnanclodratosaurus\n",
      "\n",
      "\n",
      "Iteration: 34000, Loss: 22.477910\n",
      "\n",
      "Mawspichaniaekorocimamroberax\n",
      "Inda\n",
      "Itrus\n",
      "Macaesis\n",
      "Wrosaurus\n",
      "Elaeosaurus\n",
      "Stegngosaurus\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = model(ix_to_char, char_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 结论\n",
    "\n",
    "虽然可能你英文不好，但是应该也能感觉到随着训练次数的增加，生成的字符串越来越像名字了。当然你还可以增加训练次数或者调整一些超参数，也许效果会更好。我们的数据集很小，所以能使用CPU来训练RNN。在商业项目中，数据集超大，将需要很多GPU来训练很长时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - 智能写诗\n",
    "\n",
    "智能写诗其实和我们前面的智能起名其实大同小异。智能起名的数据集是恐龙的名字，而智能写诗的数据集是一些莎士比亚的诗歌[*\"The Sonnets\"*](shakespeare.txt). 就是文档同目录下的shakespeare.txt文件。大家可能都知道莎士比亚，但是你们知道为什么称莎士比亚为老处男吗？因为它的名字取得不好——“啥是逼呀”。可能他的名字是由一个还没有训练好的模型给起的。\n",
    "\n",
    "<img src=\"images/shakespeare.jpg\" style=\"width:500;height:400px;\">\n",
    "<caption><center> 莎士比亚，好时髦，还带了个耳环 </center></caption>\n",
    "\n",
    "在智能写诗的模型中，我们使用了LSTM单元。之前因为名字很短，所以不需要RNN有很长的记忆，但是诗歌会很长，所以需要LSTM来增强RNN的记忆力。\n",
    "\n",
    "\n",
    "由于大体步骤和上面的取名是差不多的，所以就不把代码贴出来了，模型已经为大家用Keras实现了，并且已经提前为大家训练了1000个epochs。因为如果在你们电脑上训练它，需要花很长时间。执行下面的代码就可以将训练好了的模型加载起来了。仅仅是加载这个训练好了的模型都要花好几分钟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from shakespeare_utils import *\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了让大家感受一下，我们提供了下面这个代码单元，执行它可以让模型再被训练一个epoch，这个训练过程可能会花好几分钟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "31412/31412 [==============================] - 182s - loss: 1.9342   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d3a272bf60>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y, batch_size=128, epochs=1, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the beginning of your poem, the Shakespeare machine will complete it. Your input is: I love porn movie \n",
      "\n",
      "\n",
      "Here is your poem: \n",
      "\n",
      "I love porn movie so,\n",
      "it well as atle, time minke westerst goon.\n",
      "\n",
      "which i mat anthand home drechats, on the doars, filfing,\n",
      "which ach a tissungoels not thine assu, not richains ever't noh,\n",
      "that-igowedss nets-ow i tompline thou bet.\n",
      "\n",
      "as twat thee someres, un flool time me,\n",
      "if bore withtthed of hoth bate with glot,\n",
      "and high not love, beaution of dony bind whice,\n",
      "whose the latder laogeng nat threald my elose,\n",
      "vide str"
     ]
    }
   ],
   "source": [
    "# 这个工具函数会调用模型来为我们生成诗歌。运行这个代码后，下面会出现一个输入框，你在输入框里面输入一句英语，\n",
    "# 然后按回车键，模型就会接着你的这句话继续写诗。例如我们在输入框里面写上“I love porn movie ”\n",
    "generate_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，效果与商用产品比起来还差得很远，商用项目是需要很多时间人力计算力金钱的。\n",
    "\n",
    "这个智能写诗的RNN与智能起名的RNN是很相似的，当然比智能起名的要高档了一点点，主要高档了下面3个方面:\n",
    "- 使用了LSTM\n",
    "- 使用了2层神经网络\n",
    "- 使用了Keras\n",
    "\n",
    "如果你想了解智能写诗的代码，你可以去看Keras在GitHub上开源的智能写作代码: https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "1dYg0",
   "launcher_item_id": "MLhxP"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
